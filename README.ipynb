{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChromOrtho\n",
    "\n",
    "Predicting chromatographic separability for orthogonal resin pair selection in ion-exchange chromatography\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status: Prototype / Research Stage\n",
    "\n",
    "**⚠️ This model is currently under development and NOT ready for deployment or predictive use.**\n",
    "\n",
    "While the architecture is functional and can be trained, the model does not yet achieve sufficient predictive performance for practical applications. This repository serves as a research prototype and documentation of the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "ChromOrtho is a machine learning framework designed to predict separability outcomes when using two-step orthogonal resin combinations in ion-exchange chromatography. The goal is to reduce experimental trial-and-error in resin screening by forecasting which resin pairs will yield the greatest selectivity for protein separations.\n",
    "\n",
    "### Intended Capabilities (Under Development)\n",
    "\n",
    "- **Input**: Cheminformatics descriptors for two resins (Resin A and Resin B)\n",
    "- **Output**: Predicted separability score indicating how well the resin pair can separate target proteins\n",
    "- **Application**: Guide selection of orthogonal resin combinations for two-dimensional chromatography\n",
    "\n",
    "### Current Limitations\n",
    "\n",
    "- Model performance is not yet sufficient for reliable predictions\n",
    "- Limited training data availability\n",
    "- Requires further feature engineering and architecture optimization\n",
    "- Validation metrics do not meet deployment thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "### Siamese Neural Network Approach\n",
    "\n",
    "The model uses a **Siamese Multi-Layer Perceptron (MLP)** architecture designed to learn relationships between paired resin descriptors:\n",
    "\n",
    "**Architecture Components:**\n",
    "1. **Shared Encoder**: Two identical neural networks (shared weights) that independently encode descriptors for Resin A and Resin B\n",
    "2. **Embedding Layer**: Maps each resin's descriptor vector into a lower-dimensional embedding space\n",
    "3. **Interaction Features**: Combines the embeddings using:\n",
    "   - Absolute difference: `|A - B|`\n",
    "   - Element-wise product: `A × B`\n",
    "   - Element-wise sum: `A + B`\n",
    "4. **Regression Head**: Fully connected layers that predict the separability score from the combined features\n",
    "\n",
    "**Key Design Principles:**\n",
    "- **Weight sharing** ensures the model learns a consistent representation for both resins\n",
    "- **Interaction features** capture relative differences and synergies between resin properties\n",
    "- **End-to-end training** learns optimal feature representations directly from raw descriptors\n",
    "\n",
    "### Model Hyperparameters\n",
    "\n",
    "- **Embedding dimension**: 128 (default)\n",
    "- **Hidden layer size**: 256 (default)\n",
    "- **Loss function**: Mean Squared Error (MSE)\n",
    "- **Optimizer**: Adam\n",
    "- **Learning rate**: 1e-3 (default)\n",
    "- **Early stopping**: Patience of 15 epochs based on validation RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Requirements\n",
    "\n",
    "### Input Format\n",
    "\n",
    "The model expects a CSV file with paired descriptor columns:\n",
    "\n",
    "**Column Naming Convention:**\n",
    "- Features for Resin A: `<descriptor_name>_A`\n",
    "- Features for Resin B: `<descriptor_name>_B`\n",
    "- Target variable: Numeric separability score\n",
    "\n",
    "**Example Columns:**\n",
    "```\n",
    "pH_A, pH_B\n",
    "logP_A, logP_B\n",
    "Hydrophobicity_A, Hydrophobicity_B\n",
    "Charge_Density_A, Charge_Density_B\n",
    "Separability  # Target column\n",
    "```\n",
    "\n",
    "**Automatic Handling:**\n",
    "- Only complete pairs (both `_A` and `_B` present) are used\n",
    "- Columns containing `Ion_A` or `Ion_B` are automatically excluded\n",
    "- Zero-variance features are removed during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "1. **Pairing Detection**: Automatically identifies matching `_A` and `_B` columns\n",
    "2. **Data Cleaning**: Removes rows with non-finite values in features or target\n",
    "3. **Variance Filtering**: Drops features with zero variance in training set\n",
    "4. **Standardization**: StandardScaler fitted on training data (A and B stacked together)\n",
    "5. **Train/Validation Split**: Random split with configurable fraction (default 20% validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Python 3.8+\n",
    "- PyTorch (CPU or GPU)\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- numpy\n",
    "- joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create virtual environment\n",
    "# python -m venv .venv\n",
    "\n",
    "# Activate (Windows PowerShell)\n",
    "# .\\.venv\\Scripts\\Activate.ps1\n",
    "\n",
    "# Activate (Linux/Mac)\n",
    "# source .venv/bin/activate\n",
    "\n",
    "# Install dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install torch pandas scikit-learn numpy joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: For GPU support, install PyTorch with CUDA following instructions at [pytorch.org](https://pytorch.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Training a Model\n",
    "\n",
    "Basic command (PowerShell):\n",
    "\n",
    "```powershell\n",
    "python .\\siamese_from_single_csv.py `\n",
    "  --csv \"C:\\path\\to\\your_data.csv\" `\n",
    "  --target Separability `\n",
    "  --outdir \"C:\\path\\to\\outdir\" `\n",
    "  --epochs 120 `\n",
    "  --batch_size 128 `\n",
    "  --lr 1e-3 `\n",
    "  --val_frac 0.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command-Line Arguments\n",
    "\n",
    "| Argument | Required | Default | Description |\n",
    "|----------|----------|---------|-------------|\n",
    "| `--csv` | Yes | - | Path to CSV with paired features |\n",
    "| `--target` | Yes | - | Name of target column (case-insensitive) |\n",
    "| `--outdir` | Yes | - | Output directory for artifacts |\n",
    "| `--epochs` | No | 120 | Maximum training epochs |\n",
    "| `--batch_size` | No | 128 | Training batch size |\n",
    "| `--lr` | No | 1e-3 | Learning rate |\n",
    "| `--patience` | No | 15 | Early stopping patience (epochs) |\n",
    "| `--val_frac` | No | 0.2 | Validation set fraction |\n",
    "| `--emb_dim` | No | 128 | Embedding dimension |\n",
    "| `--hidden` | No | 256 | Hidden layer size |\n",
    "| `--seed` | No | 42 | Random seed |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Output\n",
    "\n",
    "During training, the script prints progress:\n",
    "\n",
    "```\n",
    "Epoch 001 | train_loss 0.1234 | val_RMSE 0.5678\n",
    "[info] New best val RMSE: 0.5678\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Output Files\n",
    "\n",
    "All outputs are saved to the specified `--outdir`:\n",
    "\n",
    "### Model Checkpoints\n",
    "\n",
    "- **`model.pt`**: Best model checkpoint (lowest validation RMSE)\n",
    "- **`model_final.pt`**: Final model after all epochs\n",
    "- **`scaler.joblib`**: Fitted StandardScaler (best checkpoint)\n",
    "- **`scaler_final.joblib`**: Fitted StandardScaler (final checkpoint)\n",
    "\n",
    "### Checkpoint Contents\n",
    "\n",
    "Each `.pt` file contains:\n",
    "- `state_dict`: Model weights\n",
    "- `in_dim`: Number of paired base features\n",
    "- `base_features`: List of base descriptor names\n",
    "- `mapping`: Dictionary mapping base names to (colA, colB) pairs\n",
    "- `target_col`: Target column name\n",
    "- `meta`: Embedding and hidden dimensions\n",
    "\n",
    "### Predictions and Metadata\n",
    "\n",
    "- **`val_predictions.csv`**: Validation set actual vs predicted values\n",
    "- **`run_summary.json`**: Training metadata including:\n",
    "  - Data paths and target column\n",
    "  - Training/validation set sizes\n",
    "  - Hyperparameters\n",
    "  - Best validation RMSE\n",
    "  - Feature information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Current Research Directions\n",
    "\n",
    "### Areas for Improvement\n",
    "\n",
    "1. **Data Augmentation**\n",
    "   - Expand training dataset with additional resin pair experiments\n",
    "   - Synthetic data generation strategies\n",
    "   - Transfer learning from related chromatography tasks\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Incorporate domain-specific resin descriptors\n",
    "   - Interaction terms beyond current set\n",
    "   - Dimensionality reduction techniques\n",
    "\n",
    "3. **Architecture Optimization**\n",
    "   - Hyperparameter tuning (embedding size, hidden layers, dropout)\n",
    "   - Alternative architectures (attention mechanisms, graph neural networks)\n",
    "   - Ensemble methods\n",
    "\n",
    "4. **Regularization**\n",
    "   - Dropout layers\n",
    "   - L1/L2 weight penalties\n",
    "   - Data-specific augmentation\n",
    "\n",
    "5. **Evaluation Metrics**\n",
    "   - Cross-validation strategies\n",
    "   - Applicability domain assessment\n",
    "   - Uncertainty quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Known Issues\n",
    "\n",
    "- **Insufficient predictive performance**: Current R² and RMSE do not meet deployment standards\n",
    "- **Limited training data**: Small dataset size limits model generalization\n",
    "- **Overfitting risk**: Model may memorize training examples rather than learn generalizable patterns\n",
    "- **Feature selection**: Optimal descriptor set not yet determined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Errors\n",
    "\n",
    "**\"No paired _A/_B features found\"**\n",
    "- Check that your CSV has matching `_A` and `_B` column suffixes\n",
    "- Ensure column names are exactly paired (case-sensitive base names)\n",
    "\n",
    "**\"--target 'X' not found\"**\n",
    "- Verify the target column exists in your CSV\n",
    "- Target matching is case-insensitive but name must match\n",
    "\n",
    "**\"Validation doesn't save best checkpoint\"**\n",
    "- Occurs if validation RMSE is not finite (e.g., empty validation set)\n",
    "- Final checkpoint is still saved\n",
    "\n",
    "**\"GPU not detected\"**\n",
    "- Script automatically uses GPU if `torch.cuda.is_available()` returns True\n",
    "- For CPU-only systems, training will proceed on CPU (slower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Future Development\n",
    "\n",
    "This prototype represents initial exploration of machine learning for orthogonal resin selection. Future work will focus on:\n",
    "\n",
    "- Collecting larger, more diverse training datasets\n",
    "- Systematic hyperparameter optimization\n",
    "- Integration with experimental validation workflows\n",
    "- Development of confidence intervals and prediction uncertainty\n",
    "- Web interface for easier model interaction\n",
    "\n",
    "**Contributions and collaborations are welcome** as we work toward a production-ready model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use ChromOrtho in your research, please cite:\n",
    "\n",
    "```\n",
    "[Citation information to be added upon publication]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## License\n",
    "\n",
    "MIT License - See LICENSE file for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Contact\n",
    "\n",
    "For questions, suggestions, or collaboration inquiries:\n",
    "- Open an issue on GitHub\n",
    "- Contact: [Your contact information]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "This work builds upon established principles in:\n",
    "- Siamese neural networks for similarity learning\n",
    "- Cheminformatics descriptor development\n",
    "- Chromatography separation science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Related Projects\n",
    "\n",
    "- **PredElute**: Predictive models for protein elution in CEX (production-ready)\n",
    "- **ProDes**: Protein descriptor calculation tools\n",
    "- **RDKit**: Cheminformatics toolkit for descriptor generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
